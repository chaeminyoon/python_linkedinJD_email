import os
import re
import time
import json
import pickle
from pathlib import Path
from datetime import datetime
from collections import Counter
from dotenv import load_dotenv

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# Setup
current_dir = Path(__file__).resolve().parent
project_root = current_dir.parent
load_dotenv(project_root / ".env")

EMAIL = os.getenv("LINKEDIN_EMAIL")
PASSWORD = os.getenv("LINKEDIN_PASSWORD")

# Cookie file path
COOKIE_FILE = project_root / "ai" / "linkedin_cookies.pkl"

# Tech skills to detect
TECH_SKILLS = [
    "Python", "SQL", "Spark", "AWS", "Azure", "GCP", "Snowflake", "Databricks",
    "Airflow", "Kafka", "Docker", "Kubernetes", "Terraform", "dbt", "ETL",
    "Java", "Scala", "Go", "Tableau", "Power BI", "Looker", "Redshift",
    "BigQuery", "PostgreSQL", "MongoDB", "Redis", "Hadoop", "Hive", "Presto",
    "Git", "CI/CD", "Linux", "REST API", "GraphQL", "Machine Learning", "ML",
    "Deep Learning", "TensorFlow", "PyTorch", "Pandas", "NumPy", "scikit-learn",
    "NLP", "Computer Vision", "Data Modeling", "Data Warehouse", "Data Lake"
]


def log(msg):
    """Print log message with timestamp."""
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] {msg}")


import random
import undetected_chromedriver as uc

def get_random_delay(min_sec=1, max_sec=3):
    """Return a random delay to simulate human behavior."""
    return random.uniform(min_sec, max_sec)


def create_driver():
    """Create Chrome driver with undetected-chromedriver for advanced detection prevention."""
    log(" ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì¤‘ (undetected-chromedriver)...")
    
    options = uc.ChromeOptions()
    
    # ê¸°ë³¸ ì˜µì…˜ë“¤
    options.add_argument("--disable-infobars")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--lang=en-US")
    
    # undetected-chromedriver ì‚¬ìš© (ìë™ìœ¼ë¡œ ë´‡ íƒì§€ ìš°íšŒ)
    driver = uc.Chrome(options=options, use_subprocess=True)
    
    log(" ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ (undetected-chromedriver)")
    return driver


def save_cookies(driver):
    """Save cookies to file after successful login."""
    COOKIE_FILE.parent.mkdir(parents=True, exist_ok=True)
    cookies = driver.get_cookies()
    with open(COOKIE_FILE, 'wb') as f:
        pickle.dump(cookies, f)
    log(f" ì¿ í‚¤ ì €ì¥ ì™„ë£Œ: {COOKIE_FILE.name}")


def load_cookies(driver):
    """Load cookies from file.
    
    Returns:
        True if cookies loaded and session is valid, False otherwise.
    """
    if not COOKIE_FILE.exists():
        log(" ì €ì¥ëœ ì¿ í‚¤ ì—†ìŒ - ë¡œê·¸ì¸ í•„ìš”")
        return False
    
    try:
        # First go to LinkedIn domain (required before adding cookies)
        driver.get("https://www.linkedin.com")
        time.sleep(get_random_delay(1, 2))
        
        with open(COOKIE_FILE, 'rb') as f:
            cookies = pickle.load(f)
        
        for cookie in cookies:
            # Remove problematic attributes
            cookie.pop('sameSite', None)
            cookie.pop('expiry', None)
            try:
                driver.add_cookie(cookie)
            except Exception:
                pass
        
        log(" ì¿ í‚¤ ë¡œë“œ ì™„ë£Œ - ì„¸ì…˜ í™•ì¸ ì¤‘...")
        
        # Verify session by checking if we can access feed
        driver.get("https://www.linkedin.com/feed/")
        time.sleep(get_random_delay(1.5, 3))
        
        # Check if we're logged in
        if "/feed" in driver.current_url or "/in/" in driver.current_url:
            log(" ì¿ í‚¤ ì„¸ì…˜ ìœ íš¨ - ë¡œê·¸ì¸ ê±´ë„ˆë›°ê¸°")
            return True
        else:
            log(" ì¿ í‚¤ ë§Œë£Œ - ë‹¤ì‹œ ë¡œê·¸ì¸ í•„ìš”")
            return False
            
    except Exception as e:
        log(f" ì¿ í‚¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


def login(driver, use_cookies=True):
    """Login to LinkedIn with cookie support.
    
    Args:
        driver: Selenium WebDriver
        use_cookies: If True, try to use saved cookies first
    """
    # Try to use saved cookies first
    if use_cookies and load_cookies(driver):
        return  # Already logged in via cookies
    
    log(" ë¡œê·¸ì¸ í˜ì´ì§€ ì ‘ì† ì¤‘...")
    driver.get("https://www.linkedin.com/login")
    
    log(" ì´ë©”ì¼/ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ ì¤‘...")
    email_field = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "username")))
    
    # ì¸ê°„ì²˜ëŸ¼ ì²œì²œíˆ íƒ€ì´í•‘
    for char in EMAIL:
        email_field.send_keys(char)
        time.sleep(random.uniform(0.05, 0.15))
    
    time.sleep(get_random_delay(0.3, 0.7))
    
    password_field = driver.find_element(By.ID, "password")
    for char in PASSWORD:
        password_field.send_keys(char)
        time.sleep(random.uniform(0.05, 0.15))
    
    time.sleep(get_random_delay(0.5, 1))
    driver.find_element(By.CSS_SELECTOR, "button[type='submit']").click()
    
    log(" ë¡œê·¸ì¸ ì™„ë£Œ ëŒ€ê¸° ì¤‘... (ë³´ì•ˆ í™•ì¸ì´ í•„ìš”í•˜ë©´ ì§ì ‘ í•´ê²°í•´ì£¼ì„¸ìš”)")
    
    # Wait longer for potential CAPTCHA solving
    WebDriverWait(driver, 120).until(EC.url_contains("/feed"))
    log(" ë¡œê·¸ì¸ ì„±ê³µ")
    
    # Save cookies for future use
    save_cookies(driver)


def get_job_description(driver):
    """Extract job description from right panel."""
    selectors = [
        ".jobs-description__content",
        ".jobs-box__html-content",
        "[data-testid='expandable-text-box']",
        ".jobs-description",
        "#job-details"
    ]
    
    for sel in selectors:
        try:
            elem = driver.find_element(By.CSS_SELECTOR, sel)
            text = elem.text.strip()
            if text and len(text) > 50:
                return text
        except:
            continue
    return ""


def extract_skills(text):
    """Extract tech skills from description."""
    found_skills = []
    text_lower = text.lower()
    
    for skill in TECH_SKILLS:
        pattern = r'\b' + re.escape(skill.lower()) + r'\b'
        if re.search(pattern, text_lower):
            found_skills.append(skill)
    
    return found_skills


def extract_experience_years(text):
    """Extract years of experience from description."""
    patterns = [
        r'(\d+)\+?\s*(?:years?|yrs?)\s*(?:of\s*)?(?:experience|exp)',
        r'(\d+)\s*-\s*\d+\s*(?:years?|yrs?)',
        r'minimum\s*(?:of\s*)?(\d+)\s*(?:years?|yrs?)'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text.lower())
        if match:
            return int(match.group(1))
    return None


def extract_education(text):
    """Extract education requirements from description."""
    text_lower = text.lower()
    
    if "phd" in text_lower or "doctorate" in text_lower:
        return "PhD"
    elif "master" in text_lower or "m.s." in text_lower or "msc" in text_lower:
        return "Master's"
    elif "bachelor" in text_lower or "b.s." in text_lower or "bsc" in text_lower:
        return "Bachelor's"
    return None


def detect_visa_sponsorship(text):
    """Detect visa sponsorship status from description."""
    text_lower = text.lower()
    
    no_patterns = [
        "no visa sponsorship", "unable to sponsor", "not sponsor",
        "cannot sponsor", "must be authorized", "must have work authorization", "no sponsorship"
    ]
    
    yes_patterns = [
        "visa sponsorship available", "will sponsor", "sponsorship available", "visa assistance"
    ]
    
    for pattern in no_patterns:
        if pattern in text_lower:
            return False
    
    for pattern in yes_patterns:
        if pattern in text_lower:
            return True
    
    return None


def parse_job_details(description):
    """Parse job description to extract structured data."""
    skills = extract_skills(description)
    required_skills = skills[:min(5, len(skills))]
    preferred_skills = skills[5:min(10, len(skills))] if len(skills) > 5 else []
    
    return {
        "experience_years": extract_experience_years(description),
        "education": extract_education(description),
        "visa_sponsorship": detect_visa_sponsorship(description),
        "required_skills": required_skills,
        "preferred_skills": preferred_skills,
        "summary": description[:300] + "..." if len(description) > 300 else description
    }


def dismiss_popups(driver):
    """Dismiss any popups or modals that may block interaction."""
    popup_selectors = [
        # ì¼ë°˜ì ì¸ ë‹«ê¸° ë²„íŠ¼ë“¤
        "button[data-tracking-control-name='public_jobs_contextual-sign-in-modal_modal_dismiss']",
        "button.artdeco-modal__dismiss",
        "button[aria-label='Dismiss']",
        "button[aria-label='ë‹«ê¸°']",
        "button.msg-overlay-bubble-header__control--new-convo-btn",
        ".artdeco-toast-item__dismiss",
        # ë©”ì‹œì§• ì˜¤ë²„ë ˆì´ ë‹«ê¸°
        "button.msg-overlay-bubble-header__control",
        # êµ¬ì§/ì´ì§ ì¤€ë¹„í•˜ê¸° íŒì—… ë‹«ê¸°
        "button.artdeco-button--circle",
        "svg[data-test-icon='close-medium']",
        # X ë²„íŠ¼ (SVG í¬í•¨)
        "button.artdeco-modal__dismiss svg",
    ]
    
    dismissed_count = 0
    for selector in popup_selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for elem in elements:
                if elem.is_displayed():
                    driver.execute_script("arguments[0].click();", elem)
                    dismissed_count += 1
                    time.sleep(0.3)
        except:
            continue
    
    # ESC í‚¤ë¡œ ëª¨ë‹¬ ë‹«ê¸° ì‹œë„
    try:
        from selenium.webdriver.common.keys import Keys
        body = driver.find_element(By.TAG_NAME, "body")
        body.send_keys(Keys.ESCAPE)
        time.sleep(0.5)
    except:
        pass
    
    if dismissed_count > 0:
        log(f" íŒì—… {dismissed_count}ê°œ ë‹«ìŒ")
    
    return dismissed_count


def scrape_jobs(driver, search_url, max_jobs=25):
    """Scrape job listings by clicking each one."""
    log(f" ê²€ìƒ‰ í˜ì´ì§€ ì ‘ì† ì¤‘...")
    log(f"   URL: {search_url}")
    driver.get(search_url)
    
    delay = get_random_delay(2, 4)
    log(f" í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ({delay:.1f}ì´ˆ)...")
    time.sleep(delay)
    
    # íŒì—…/ëª¨ë‹¬ ë‹«ê¸°
    dismiss_popups(driver)
    time.sleep(0.5)
    
    # Debug: Print current URL
    log(f" í˜„ì¬ URL: {driver.current_url}")
    
    # Try multiple selectors for job list
    job_list_selectors = [
        ".jobs-search-results-list",
        ".scaffold-layout__list",
        ".jobs-search-results__list",
        "[data-results-list-container]"
    ]
    
    job_list_found = False
    for selector in job_list_selectors:
        try:
            log(f" ì¡ ë¦¬ìŠ¤íŠ¸ ì°¾ëŠ” ì¤‘... (selector: {selector})")
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, selector))
            )
            log(f" ì¡ ë¦¬ìŠ¤íŠ¸ ë°œê²¬: {selector}")
            job_list_found = True
            break
        except:
            log(f"    {selector} ì—†ìŒ")
            continue
    
    if not job_list_found:
        log("ì¡ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        log("í˜ì´ì§€ ì†ŒìŠ¤ ì¼ë¶€:")
        # Print a snippet of the page source for debugging
        page_source = driver.page_source
        if "sign-in" in page_source.lower() or "login" in page_source.lower():
            log("ë¡œê·¸ì¸ì´ í•„ìš”í•œ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤")
        if "no results" in page_source.lower():
            log("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤")
        
        # Try to find any job-related elements
        all_elements = driver.find_elements(By.CSS_SELECTOR, "[class*='job']")
        log(f"   'job' ê´€ë ¨ ìš”ì†Œ ê°œìˆ˜: {len(all_elements)}")
        
        # Save screenshot for debugging
        screenshot_path = project_root / "data" / "debug_screenshot.png"
        screenshot_path.parent.mkdir(exist_ok=True)
        driver.save_screenshot(str(screenshot_path))
        log(f"   ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_path}")
        
        return []
    
    # Try multiple selectors for job cards
    job_card_selectors = [
        ".jobs-search-results-list__list-item",
        ".scaffold-layout__list-item",
        ".jobs-search-results__list-item",
        "li[data-occludable-job-id]"
    ]
    
    jobs = []
    processed = set()
    
    for _ in range(max_jobs):
        cards = []
        for selector in job_card_selectors:
            cards = driver.find_elements(By.CSS_SELECTOR, selector)
            if cards:
                log(f"ì¡ ì¹´ë“œ {len(cards)}ê°œ ë°œê²¬ (selector: {selector})")
                break
        
        if not cards:
            log("ì¡ ì¹´ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            break
        
        found_new = False
        for card in cards:
            try:
                link = card.find_element(By.CSS_SELECTOR, "a[href*='/jobs/view/']")
                job_id = link.get_attribute("href").split("/jobs/view/")[1].split("/")[0].split("?")[0]
                
                if job_id in processed:
                    continue
                
                processed.add(job_id)
                found_new = True
                
                log(f"ê³µê³  í´ë¦­ ì¤‘... (ID: {job_id})")
                driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", card)
                time.sleep(get_random_delay(0.3, 0.8))
                driver.execute_script("arguments[0].click();", link)
                time.sleep(get_random_delay(1.5, 3))
                
                title = link.text.strip().split("\n")[0]
                
                try:
                    company = card.find_element(By.CSS_SELECTOR, ".artdeco-entity-lockup__subtitle").text.strip()
                except:
                    company = ""
                
                try:
                    location = card.find_element(By.CSS_SELECTOR, ".artdeco-entity-lockup__caption").text.strip()
                except:
                    location = ""
                
                log(f"ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì¤‘... ({title})")
                description = get_job_description(driver)
                
                if description:
                    log(f"Description ê¸¸ì´: {len(description)}ì")
                else:
                    log(f"Description ì¶”ì¶œ ì‹¤íŒ¨")
                
                parsed = parse_job_details(description)
                
                job_data = {
                    "id": job_id,
                    "title": title,
                    "company": company,
                    "location": location,
                    "url": f"https://www.linkedin.com/jobs/view/{job_id}/",
                    "description": description,
                    **parsed
                }
                
                jobs.append(job_data)
                log(f"âœ“ [{len(jobs)}/{max_jobs}] {title} @ {company} | ìŠ¤í‚¬: {len(parsed['required_skills'])+len(parsed['preferred_skills'])}ê°œ")
                
                if len(jobs) >= max_jobs:
                    break
                    
            except Exception as e:
                log(f"ì—ëŸ¬: {str(e)[:50]}")
                continue
        
        if not found_new or len(jobs) >= max_jobs:
            break
            
        log("ìŠ¤í¬ë¡¤ ë‹¤ìš´...")
        try:
            scroll_container = driver.find_element(By.CSS_SELECTOR, ".jobs-search-results-list")
            scroll_amount = random.randint(300, 600)
            driver.execute_script(f"arguments[0].scrollTop += {scroll_amount};", scroll_container)
        except:
            scroll_amount = random.randint(300, 600)
            driver.execute_script(f"window.scrollBy(0, {scroll_amount});")
        time.sleep(get_random_delay(0.8, 1.5))
    
    return jobs


def generate_report_data(jobs):
    """Generate data for report template."""
    all_skills = []
    for job in jobs:
        all_skills.extend(job.get("required_skills", []))
        all_skills.extend(job.get("preferred_skills", []))
    
    skill_counts = Counter(all_skills)
    max_count = max(skill_counts.values()) if skill_counts else 1
    
    skill_chart_data = [
        {"name": skill, "count": count, "percentage": round((count / max_count) * 100)}
        for skill, count in skill_counts.most_common(15)
    ]
    
    top_skills = [item["name"] for item in skill_chart_data[:5]]
    trending_skills = [item["name"] for item in skill_chart_data[5:10]]
    
    now = datetime.now()
    
    return {
        "report_date": now.strftime("%B %d, %Y"),
        "year": now.year,
        "total_jobs": len(jobs),
        "skill_chart_data": skill_chart_data,
        "top_skills": top_skills,
        "trending_skills": trending_skills,
        "recommendation": generate_recommendation(top_skills, trending_skills),
        "jobs": jobs
    }


def generate_recommendation(top_skills, trending_skills):
    """Generate recommendation text based on skills analysis."""
    if not top_skills:
        return "No specific recommendations available. Check back with more job data."
    
    rec = f"Based on today's analysis, focus on mastering {', '.join(top_skills[:3])} as they appear most frequently in job postings."
    
    if trending_skills:
        rec += f" Additionally, consider learning {trending_skills[0]} to stand out from other candidates."
    
    return rec


def save_data(jobs, report_data, filename_prefix="linkedin"):
    """Save jobs and report data to JSON files."""
    output_dir = project_root / "data"
    output_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    jobs_path = output_dir / f"{filename_prefix}_jobs_{timestamp}.json"
    with open(jobs_path, "w", encoding="utf-8") as f:
        json.dump(jobs, f, ensure_ascii=False, indent=2)
    
    report_path = output_dir / f"{filename_prefix}_report_{timestamp}.json"
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
    
    log(f"{len(jobs)}ê°œ ì¡ ì €ì¥: {jobs_path}")
    log(f"ë¦¬í¬íŠ¸ ë°ì´í„° ì €ì¥: {report_path}")
    
    return jobs_path, report_path


def run_scraper(keywords="Data Engineer", location="Canada", max_jobs=25):
    """
    Run the scraper programmatically (for pipeline automation).
    Returns a result dictionary instead of using input() for waiting.
    """
    search_url = f"https://www.linkedin.com/jobs/search/?keywords={keywords.replace(' ', '%20')}&location={location}&f_TPR=r86400&sortBy=DD"
    
    log("=" * 50)
    log("LinkedIn Job Scraper (Automated Mode)")
    log(f"Keywords: {keywords}")
    log(f"Location: {location}")
    log("=" * 50)
    
    driver = create_driver()
    driver.maximize_window()
    
    result = {
        'success': False,
        'jobs_count': 0,
        'jobs_file': None,
        'report_file': None,
        'error': None
    }
    
    try:
        login(driver)
        
        log("")
        log("=" * 50)
        log("Starting job scraping...")
        log("=" * 50)
        
        jobs = scrape_jobs(driver, search_url, max_jobs=max_jobs)
        
        if jobs:
            log("")
            log("=" * 50)
            log("Generating report...")
            report_data = generate_report_data(jobs)
            jobs_path, report_path = save_data(jobs, report_data)
            
            result['success'] = True
            result['jobs_count'] = len(jobs)
            result['jobs_file'] = str(jobs_path)
            result['report_file'] = str(report_path)
            
            log("")
            log("=== Summary ===")
            log(f"Total jobs: {report_data['total_jobs']}")
            log(f"Top skills: {', '.join(report_data['top_skills'])}")
            log(f"Trending skills: {', '.join(report_data['trending_skills'])}")
        else:
            result['error'] = "No jobs found"
            log("")
            log("No jobs collected")
            
    except Exception as e:
        result['error'] = str(e)
        log(f"Error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        log("")
        log("Closing browser...")
        driver.quit()
    
    return result

def main(auto_mode=False):
    """Main function for standalone execution.
    
    Args:
        auto_mode: If True, skip input() wait at the end (for pipeline automation)
    """
    # ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
    keywords_list = ["Data Engineer", "AI Engineer", "Data Scientist"]
    location = "Canada"
    max_jobs_per_keyword = 15  # ê° í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 15ê°œì”© (ì´ 45ê°œ ëª©í‘œ)
    
    log("=" * 50)
    log("LinkedIn Job Scraper ì‹œì‘")
    log(f"ê²€ìƒ‰ì–´: {', '.join(keywords_list)}")
    log(f"ì§€ì—­: {location}")
    log(f"í‚¤ì›Œë“œë‹¹ ìµœëŒ€: {max_jobs_per_keyword}ê°œ")
    if auto_mode:
        log("Mode: Auto (íŒŒì´í”„ë¼ì¸ ì‹¤í–‰)")
    log("=" * 50)
    
    driver = create_driver()
    driver.maximize_window()
    
    exit_code = 0
    all_jobs = []
    processed_ids = set()  # ì¤‘ë³µ ë°©ì§€
    
    try:
        login(driver)
        
        for keyword in keywords_list:
            log("")
            log("=" * 50)
            log(f"ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í•‘: '{keyword}'")
            log("=" * 50)
            
            search_url = f"https://www.linkedin.com/jobs/search/?keywords={keyword.replace(' ', '%20')}&location={location}&f_TPR=r86400&sortBy=DD"
            
            jobs = scrape_jobs(driver, search_url, max_jobs=max_jobs_per_keyword)
            
            # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ì¶”ê°€
            new_jobs = 0
            for job in jobs:
                if job['id'] not in processed_ids:
                    processed_ids.add(job['id'])
                    job['search_keyword'] = keyword  # ì–´ë–¤ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ë˜ì—ˆëŠ”ì§€ í‘œì‹œ
                    all_jobs.append(job)
                    new_jobs += 1
            
            log(f"âœ“ '{keyword}': {new_jobs}ê°œ ìƒˆ ê³µê³  ì¶”ê°€ (ì¤‘ë³µ ì œì™¸)")
            
            # í‚¤ì›Œë“œ ê°„ ë”œë ˆì´ (ë´‡ íƒì§€ ë°©ì§€)
            if keyword != keywords_list[-1]:
                delay = get_random_delay(3, 5)
                log(f" ë‹¤ìŒ ê²€ìƒ‰ê¹Œì§€ {delay:.1f}ì´ˆ ëŒ€ê¸°...")
                time.sleep(delay)
        
        if all_jobs:
            log("")
            log("=" * 50)
            log("ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
            report_data = generate_report_data(all_jobs)
            save_data(all_jobs, report_data)
            
            log("")
            log("=== ê²°ê³¼ ìš”ì•½ ===")
            log(f"ì´ ìˆ˜ì§‘ ê³µê³ : {report_data['total_jobs']}ê°œ")
            log(f"Top ìŠ¤í‚¬: {', '.join(report_data['top_skills'])}")
            log(f"Trending ìŠ¤í‚¬: {', '.join(report_data['trending_skills'])}")
        else:
            log("")
            log("ìˆ˜ì§‘ëœ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤")
            log("- ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ê±°ë‚˜")
            log("- í˜ì´ì§€ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            log("- data/debug_screenshot.png ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”")
            exit_code = 1
            
    except Exception as e:
        log(f"ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        exit_code = 1
    finally:
        log("")
        log("ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘...")
        if not auto_mode:
            input("Enter í‚¤ë¥¼ ëˆ„ë¥´ë©´ ë¸Œë¼ìš°ì €ê°€ ë‹«í™ë‹ˆë‹¤...")
        driver.quit()
    
    return exit_code


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='LinkedIn Job Scraper')
    parser.add_argument('--auto', action='store_true', 
                        help='Auto mode: skip input() wait at the end (for pipeline)')
    
    args = parser.parse_args()
    
    exit_code = main(auto_mode=args.auto)
    exit(exit_code)
